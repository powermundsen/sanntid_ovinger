3: Reasons for concurrency and parallelism


What is concurrency? 
	- Dette er et egenskap et system har ved at det kan utføre flere beregninger/handlinger samtidig og kanskje også få disse til å sammarbeide. Dette 
	kan gjøres i flere kjerner på samme chip, på forhåndsplanlagte tråder i samme prosessor eller på fysisk separate prosessorer. 


		Executing two tasks concurrently means that individual steps of both tasks are executed in an interleaved fashion. If you disregard parallelism, you can assume that only one statement is executed at any point in time, but you have (a priori)no guarantee which task gets to execute the next step.

		This is useful in some regards:

		Clearer programming of independent tasks in one program.
		Allows to deal with IO while computing (e.g. in GUI).
		Allows for execution of more than one program at a time (concurrency on OS level.
		
		Some of the main challenges are:

		Maintain data consistency.
		Avoid deadlocks and livelocks.
		Determine precise semantics of concurrent processes.
		Determine static properties that ensure correctness.

What is parallelism? 
	- Dette er når to oppgaver utføres parallelt og ikke har noe med hverandre å gjøre (de er ikke avhengig av hverandre for å løse hverandre).

		Executing two tasks in parallel means that statements are executed at the same time. This is mainly useful for:

		Improve system throughput by executing programs in parallel (e.g. on multi core systems).
		Improve runtime of individual programs by utilising multiple CPUs at once.
		Utilise IO on many machines (e.g. distributed databases).
		Key challenges include:

		Partition problems that allow and develop algorithms that can employ parallelism.
		Minimise dependencies and communication among the computation units.
		All the problems brought by concurrency: at least from the point of view of memory, parallel programs look like concurrent ones due to serialisation of memory accesses.
		Deal with sub-optimal hardware support.

What's the difference?
	- Concurrency: A condition that exists when at least two threads are making progress. A more generalized form of parallelism that can include time-slicing as a form of virtual parallelism.

	Parallelism: A condition that arises when at least two threads are executing simultaneously.

Why have machines become increasingly multicore in the past decade?
	- Fordi det er lettere å lage flere kjerner som kan jobbe med mer samtidig enn det er å lage en prosessor som jobber raskere alene!

What kinds of problems motivates the need for concurrent execution?
(Or phrased differently: What problems do concurrency help in solving?)
	- USIKKER! Men kan det ha noe med å løse "uløselige" problmer? 

Does creating concurrent programs make the programmer's life easier? Harder? Maybe 
both? (Come back to this after you have worked on part 4 of this exercise)
		

What are the differences between processes, threads, green threads, and coroutines?

	Preemptive scheduling: The preemptive scheduling is prioritized. The highest priority process should always be the process that is currently utilized.

	Non-Preemptive scheduling: When a process enters the state of running, the state of that process is not deleted from the scheduler until it finishes its service time.
		
		Process: 
			"Abstraksjon for å eie hele maskinen"
			-OS-managed
			-Each has its own virtual address space
			-Can be interrupted (preempted) by the system to allow another process to run
			-Can run in parallel with other processes on different processors
			-The memory overhead of processes is high (includes virtual memory tables, open file handles, etc)
			-The time overhead for creating and context switching between processes is relatively high

		Thread: 
			"Illusjonen eller realiteten av induviduelle sekvenser med instruksjoner"
			-OS-managed
			-Each is "contained" within some particular process
			-All threads in the same process share the same virtual address space
			-Can be interrupted by the system to allow another thread to run
			-Can run in parallel with other threads on different processors
			-The memory and time overheads associated with threads are smaller than processes, but still non-trivial
			(For example, typically context switching involves entering the kernel and invoking the system scheduler.)

		
		Green Thread: 
			These are user-space projections of the same concept as threads, but are not OS-managed. Probably not truly concurrent, except in the sense that there may be multiple worker threads or processes giving them CPU time concurrently, so probably best to consider this as interleaved or multiplexed.
			In computer programming, green threads are threads that are scheduled by a runtime library or virtual machine (VM) instead of natively by the underlying operating system. Green threads emulate multithreaded environments without relying on any native OS capabilities, and they are managed in user space instead of kernel space, enabling them to work in environments that do not have native thread support.


		Coroutines: 
			-Some people use "coroutine" and "cooperative thread" more or less synonymously
				I do not prefer this usage
			- Some coroutine implementations are actually "shallow" cooperative threads; yield can only be invoked by the "coroutine entry procedure"
			- The shallow (or semi-coroutine) version is easier to implement than threads, because each coroutine does not need a complete stack (just one frame for the entry procedure)
			- Often coroutine frameworks have yield primitives that require the invoker to explicitly state which coroutine control should transfer to

			Coroutines are computer program components that generalize subroutines for nonpreemptive multitasking, by allowing multiple entry points for suspending and resuming execution at certain locations. Coroutines are well-suited for implementing more familiar program components such as cooperative tasks, exceptions, event loop, iterators, infinite lists and pipes.




Which one of these do pthread_create() (C/POSIX), threading.Thread() (Python), go (Go) create?
	- Green Tread? 

How does pythons Global Interpreter Lock (GIL) influence the way a python Thread behaves?
	- In CPython, the global interpreter lock, or GIL, is a mutex that prevents multiple native threads from executing Python bytecodes at once. This lock is necessary mainly because CPython's memory management is not thread-safe. (However, since the GIL exists, other features have grown to depend on the guarantees that it enforces.)

With this in mind: What is the workaround for the GIL (Hint: it's another module)?
What does func GOMAXPROCS(n int) int change?
	- ?